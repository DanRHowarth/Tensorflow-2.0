{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf2_Notebook 1_Hello World.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "zr8UKFBOkucP",
        "hO5ms0zgTb7j",
        "GFHhC6fkBUYu",
        "20NpMIjtJL94",
        "8K6ScKmR4vvi",
        "KknUS21-7rpc",
        "_hwIqC3b6mRr",
        "gMhDLmZK7HW2",
        "BPF0Hul7hN7j"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanRHowarth/Tensorflow-2.0/blob/master/tf2_Notebook_1_Hello_World.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFwSIbJn_jdX",
        "colab_type": "text"
      },
      "source": [
        "# tensorflow 2.0: Notebook 1: 'Hello World' Deep Learning with Tensorflow 2.0  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr8UKFBOkucP",
        "colab_type": "text"
      },
      "source": [
        "## 1. Introduction to the Notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNBCNGDOBBNP",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 What we will cover today\n",
        "\n",
        "**What will we cover today?**\n",
        "* We will: provide an introduction to the core Deep Learning Concepts;  provide an introduction to `Tensorflow 2.0`; and, provide an introduction to Computer Vision. \n",
        "\n",
        "\n",
        "**How will we go about it?**\n",
        "* We will have four sessions today. The first will provide a basic introduction to Deep Learning and `Tensorflow 2.0`. The second will go in to more detail about Computer Vision. \n",
        "\n",
        "* Session Three will cover Transfer Learning, an important technique for developing state of the art models. Session Four will be an opportunity to put together everything you have learned and develop your own models.\n",
        "\n",
        "\n",
        "**How are the sessions structured?**\n",
        "* We will provide some introductory concepts and instructions at the start of session. You will then work through a notebook that will cover the topics for that session. You can do this individually or in conjunction with others around you. We will answer questions and support throughout the session. At the end of the session, we will summarise what we have learnt. \n",
        "* In addition, we are preparing some advanced topic notebooks that you can work through ifollowing today's session. They will build on the initial notebooks and provide an insight into the lower level tensorflow API.\n",
        "\n",
        "\n",
        "**Can you set out all the notebooks?**\n",
        "* Session 1: 'Hello World' Deep Learning with Tensorflow 2.0 (this notebook)\n",
        "* Session 2: Computer Vision with CNNs \n",
        "* Session 3: Transfer Learning \n",
        "\n",
        "**And the Advanced Notebooks?**\n",
        "* Advanced 1: Model and Layers \n",
        "* Advanced 2: Custom Training Loops \n",
        "* Advanced 3: Data Pipelines and Augmentation \n",
        "* Advanced 4: Tensors \n",
        "\n",
        "**What will you not cover?**\n",
        "* There are certain things that we can't cover today because of time. We won't cover the maths behind Deep Learning (calculus and Linear Algebra). There won't be detailed coverage of the topics, although we will cover the main concepts and provide some follow-up reading. \n",
        "* And, we won't have exciting datasets. We will use benchmark (but slightly dated) datasets that are available via the Tensorflow 2.0 API. Our focus is on concepts and code, and this means using datasets that are available to everyone and can be trained on easily. \n",
        "\n",
        "**One final thing...**\n",
        "* The Session 1 - 3 tutorials are based on tutorials published on the Tensorflow 2.0 website [link](https://www.tensorflow.org/beta). We have provided a lot more material than is in those tutorials, and the advanced tutorials are new."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO5ms0zgTb7j",
        "colab_type": "text"
      },
      "source": [
        "## 2. Introduction to this Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJs-EQZ9TgI9",
        "colab_type": "text"
      },
      "source": [
        "**What will we cover in this notebook?**\n",
        "* This notebook will introduce the core concepts of Deep Learning. We will also start coding straightaway with `Tensorflow 2.0`.\n",
        "* Let's start by loading the necessary libraries, and introducting the problem we are going to work on. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iWAlt02EuWS",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Loading the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUvIIN0Q-pWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we need to install tensorflow 2.0 on the google cloud notebook we have opened\n",
        "!pip install -q tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeOGXI_2_Znq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports future functionality that might modify modules otherwise used and make them incompatible in the future. \n",
        "# We are future proofing by importing modules that modify or replace exising modules that we may have used now \n",
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uECyzBrF5NL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3ZbUiSRIvoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GwCkCHWI5CC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's print out the version we are using \n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUWZuDWUBRw5",
        "colab_type": "text"
      },
      "source": [
        "###2.2 Introduction to our problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPNK3F35Bg4v",
        "colab_type": "text"
      },
      "source": [
        "**What problem are we trying to solve?**\n",
        "* We will use the Fashion MNIST dataset. This is a dataset of images of clothes (you will see what the data looks like soon). The task is to train a model on this dataset so that when the model sees a new image of clothes, it classifies it correctly.\n",
        "* The dataset is quite large - 60,000 images to train on and 10,000 to test on. However, the images are small and therefore its possible for us to train on easily. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFHhC6fkBUYu",
        "colab_type": "text"
      },
      "source": [
        "## 3. Deep Learning Conceptual Introduction "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFXSBQa_GQoj",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/DanRHowarth/Tensorflow-2.0/blob/master/Notebook%201%20-%20Deep%20Learning%20Concepts.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MumBZIEdA-2w",
        "colab_type": "text"
      },
      "source": [
        "**What are the main concepts in Deep Learning?**\n",
        "\n",
        "* The diagram above shows what I think are the core concepts of Deep Learning (in a supervised learning context). We will assume we are using a training set where we know the matching input and output values.\n",
        "* We have a dataset that is an input to the deep learning model. We need to define how we will **process**  **input** data, if at all.\n",
        "* We will define how we build our **model**. These user-defined concepts are our *hyperparameters* (things that the deep learning practioner sets). They create the total number of *parameters* that will be trained to map our input data to output values. *Parameters* are things learned by the model - they represent the *learning* in deep learning. In the case of deep learning, the parameters and weights and biases of the model. \n",
        "* We will then use our model to train on the data. We will pass our data through the model in a **forward pass**; this will provide an output value by performing mathematical operations at each of stage of our model. \n",
        "* We will **assess the performance** of our model by comparing this output value with the actual value to generate a *loss* value. The loss will represent the difference between the model's performance and the actual dataset. We will also measure the performance with other *metrics*\n",
        "* We will then perform a **backward pass**, where we use the loss value to update our parameters using an *optimizer*. This optimizer performs a version of *backpropogation* and the parameters, so that their contribution to the overall loss is identified and corrected to some extent during each training loop.\n",
        "* This training loop is repeated until the parameters are sufficiently updated that they are able to accurately map the input data to the output values.  \n",
        "\n",
        "**Is that it?**\n",
        "\n",
        "* There are other facets to deep learning and its hard to keep things at a high level and not delve in to the details.\n",
        "* Throughout the notebooks, we will update the chart above with more detail as we learn it. This should hopefully start to build knowledge around the key concepts, and let you see how new things fit in to an overall framework.\n",
        "\n",
        "**Does Tensorflow 2.0 cover all these areas?**\n",
        "\n",
        "* Yes. Once we have learnt a new piece of code, we will update the chart above to see where that bit of code fits. Again, hopefully this will help you learn how the code implements the concepts more easily.\n",
        "\n",
        "**TODO: Update chart. Add in additional ML words**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20NpMIjtJL94",
        "colab_type": "text"
      },
      "source": [
        "## 4. Data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLMXuczcEZQf",
        "colab_type": "text"
      },
      "source": [
        "**What do we need to think about with regard to our data?**\n",
        "* We need to understand what our input and output data is. Given the problem we are trying to solve, does the data help us? \n",
        "* For this notebook, we are classifying images. This means that our input data are a series of images and our output data are the classes we want to use to classify images. \n",
        "* We are using a well known dataset so can expect the data to be complete and not corrupted, but you might need to check this when you are using different datasets. \n",
        "\n",
        "**What about *processing inputs*, the first conceptual block above?**\n",
        "* Our aim is to ensure the data is a suitable state to model, and that the data is loaded into the training loop in a way that maximises learning. \n",
        "* There are a few things we might do to ensure that our data is processed correctly. For now, we will begin by understanding the `size` and `shape` of the data, and by `rescaling` it so that it can be modelled effectively. We will also split our data into `training and test sets`.\n",
        "* Let's start by going through some code to load the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpjS9o5DI9H2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using a preloaded dataset \n",
        "fashion_mnist = keras.datasets.fashion_mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm0yM95cLMSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load_data() is used to load a keras_dataset\n",
        "# it returns two sets of tuples that provides data set arrays and labels, one for training and one for testing data \n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU8igoKptbYD",
        "colab_type": "text"
      },
      "source": [
        "**What did we just do and why?**\n",
        "* We loaded and split our data into `training and test sets`. Each set has our images and a corresponding array of labels.\n",
        "* In machine learning, it is good practice to have both a training and a testing set. \n",
        "* We will train our model on the training set, meaning we will compare the predicts our model makes against known results to update the model parameters. This will help improve the model. \n",
        "* At the end of training, we will use our model to predict results on our test data. This is input data that the model has not been trained on, and the corresponding output labels that the model doesn't see. We will then compare the results of our model with the actual labels. \n",
        "* The ability of the model to predict accurately using unseen data is the benchmark that determines how effective the model.\n",
        "\n",
        "**What are our inputs and outputs?**\n",
        "* In this instance, we returned four `numpy` arrays:\n",
        "  * train_images\n",
        "  * train_labels\n",
        "  * test_images\n",
        "  * test_labels\n",
        "* Now that we have loaded and split our data, we can explore the `size` and `shape` of the dataset, and preprocess it as required. \n",
        "\n",
        "\n",
        "**TODO: Diagram showing training and testing split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHUpnZWLtmwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets start by looking at the size of the train and test sets\n",
        "\n",
        "# lets get the shape\n",
        "train_images.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clW0BBu7u3F_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the same info for our test set\n",
        "test_images.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0QFldT2rzwS",
        "colab_type": "text"
      },
      "source": [
        "**What does this show?**\n",
        "* We have 60,000 images in our train set, and 10000 images in our test set. It is common practice to have significantly more training than testing images.\n",
        "* We can also see that the shape of the data for each image is 28 x 28. This is 28 rows by 28 columns. Compared to other image data you might model in the future, this is small and is why this is a good dataset for a tutorial. \n",
        "* If the training and testing image sizes were different, we would need to get them to the same size to pass in to the model. \n",
        "* When we create our model we will need to pass the shape information to the model's first layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLl6_fHPtsUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## let's look at our training labels\n",
        "\n",
        "# we can see that there are 10 labels\n",
        "np.unique(train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdyG6Dbruxmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and that the train and test labels correspond to the size of the train and test sets\n",
        "len(train_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FYdZpkJvzCL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnqZzBeS2jP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## we can see that our labels are just numbers. We need to match them to description of the image\n",
        "\n",
        "# create a list of the labels\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dssU9T2wjcO",
        "colab_type": "text"
      },
      "source": [
        "**We also need to preprocess the images by rescaling them. Why?** \n",
        "* As you will see below, all of our image arrays are between the values of 0 and 255, with each value corresponding to a colour. We need to rescale them so that they are all between 0 and 1.\n",
        "* The high level reason for this is that this helps with training the model. The model will update its parameters more effectively if all the input values are on the same scale, and in a defined range between 0 and 1. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m00J824XMSNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## lets look at an image and use matplotlib to plot the array values\n",
        "\n",
        "# converts the numpy array to an image and displays it\n",
        "plt.imshow(train_images[0])\n",
        "# displays the image values \n",
        "plt.colorbar()\n",
        "# displays the chart only - comment out to see what info it omits\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWRZH_fNrZJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets preprocess\n",
        "train_images = train_images / 255.0 \n",
        "\n",
        "# we need to do the same to the test and training set\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yxuHSFpxKzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets look again at the pixel range \n",
        "\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoD5Z1zI1YJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we have the class names, lets look at a selection of the images\n",
        "\n",
        "# sets the size of the overall display for our images\n",
        "plt.figure(figsize=(10,10))\n",
        "# loops through the first 25 images\n",
        "for i in range (25):\n",
        "  # sets a location for each of the images\n",
        "  plt.subplot(5,5,i+1)\n",
        "  # removes the axis lables\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  # displays the image \n",
        "  plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "  # plots the label, mapping the label to our list of clothing items\n",
        "  plt.xlabel(class_names[train_labels[i]])\n",
        "# displays the image and label\n",
        "plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSQmofQjmMG5",
        "colab_type": "text"
      },
      "source": [
        "**So, what did we cover in this section?**\n",
        "* We looked at understanding and processing our data. \n",
        "* Specifically, we looked at data size and shape, rescaling our data, and splitting our data into training and test sets.\n",
        "\n",
        "**How does it add to our existing knowledge?**\n",
        "* This starts to add to the high level concepts we introduced in section 3.\n",
        "\n",
        "**What else can I learn to improve my knowledge?**\n",
        "* We touch briefly on how each array value represents a colour value. We will look at this in more detail in the next notebook. \n",
        "* There is more to learn on splitting training and tests. In later notebooks, we will add a validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K6ScKmR4vvi",
        "colab_type": "text"
      },
      "source": [
        "## 5. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKcp3svI-pNs",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://cdn-images-1.medium.com/max/1200/0*0mia7BQKjUAuXeqZ.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EnOP4qKEcvv",
        "colab_type": "text"
      },
      "source": [
        "**What is a deep learning model?**\n",
        "* The image above ([credit](http://cs231n.github.io/neural-networks-1/)) sets out the main components of a deep learning model: \n",
        "  * an `input layer` that takes our data\n",
        "  * an `output layer` that returns a value \n",
        "  * `hidden layers` that generate the parameters that will learn the mapping between inputs and outputs. * The hidden layers provide the depth to the model. \n",
        "  * the arrows, which represent the `connections` between the layers. Each arrow shows how a value from one layer will be passed to another. As the value is passed from one layer to another, its is multiplied by a **`weight`**, which is a **`learned parameter`**, to return a different value.\n",
        "  * **`nodes`** in each of the layers that represent the **`activation functions`** within the model. An activation function is where the product of all the inputs and weights into the node are summed, along with a bias term (biases are also a learned parameter) and where an output value is returned dependent on the sort of activation function we choose. \n",
        "* We configure the model by setting the number of layers and defining what each layers does prior to training the model on our data.  \n",
        "\n",
        "**How do we build Deep Learning Models?**\n",
        "* In order to build a deep learning model, we can define the following hyperparameters: the number of layers, the size of layer, the type of layer, and the type of activation function in each layer.\n",
        "\n",
        "**So layers are important?** \n",
        "* The building block of a neural network is the layer. Layers make up models as we can see from the diagram above.\n",
        "* Layers extract representations from the data fed into them (taken from 'first CNN tutorial'). They contain the parameters (**`weights`** and **`biases`**) that our model learns to make predictions.  \n",
        "* Given that the layer parameters are learned during train, our task in building a deep learning model is to ensure there is sufficient **`capacity`** in the model to learn the representations required to map between the input and output data. \n",
        "* This capacity is provided by the number of layers and the size of each layer (the number of parameters in each layer). \n",
        "* The larger the capacity, the more representations we can have. But we need to trade off the number of parameters with the dangers of overfitting, or developing a model that is too specific to the training data and doesn't perform well on unseen data. (We will look at overfitting later) \n",
        "\n",
        "**What about layer type?**\n",
        "* Layers perform a set of mathematical operations on the data. The sort of operations change depending on what representations we want and define with the code. We will use **`dense`**, or **`fully connected`**, layers in this tutorial, and **`convolutional`** layers in the following tutorials. \n",
        "* These different layers are best suited to different tasks within the mode, as we will see in these notebooks.\n",
        "\n",
        "**So its just layers?**\n",
        "* No, we also define **`activation functions.`** These take input values (which will be the product of the output values of the previous layer and the weight parameter), sum them together and produce an output value. The value returned will depend on the type of activation function selected. \n",
        "* The output layer will usually have a different activation function to the rest of the model, one that will be based on the required output of the model.\n",
        "\n",
        "**That's a lot to take in...**\n",
        "* It is. But these tutorials will set out some good guidelines. We will learn that:\n",
        "  * Convolutional layers are good for extracting representations from image data, while dense layers are better suited for classifying those representation by mapping them to the outputs.\n",
        "  * An effective activation function for the dense hidden layers is **`Relu`**. \n",
        "  * An effective activation function for the output layer is **`Softmax`**.\n",
        "\n",
        "**How do we build a model in Tensorflow 2.0?**\n",
        "* There are a number of ways to build models in **`tensorflow 2.0`**. \n",
        "* Most straightforward is using the **`sequential API`**. We will use this to develop our models. \n",
        "* We can also use the **`functional API`** and a technique known as **`sub-classing`**. We will explore these in the advanced notebooks as they offer benefits over the sequential API if you want to build certain sorts of model.\n",
        "* We will develop out first model below. As you will see - and this holds true for a lot of deep learning - the code required to implement complex ideas that require a lot of explanation is pretty small."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccnLxEfh6cW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# here we instantiate a Sequential model \n",
        "# note that are passing in the layers as a list\n",
        "model = keras.Sequential([\n",
        "    # the input / flatten layer changes the input shape to a 1D array of 28x28 size \n",
        "    keras.layers.Flatten(input_shape=(28,28)),\n",
        "    # here we define the hidden / Dense layer. We specify the number of nodes - 128 - and the activatin function - relu\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    # we define the output / Dense layer with 10 nodes for 10 classes, and a softmax to return an array of 10 probability scores that sum to 1\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R29CrREF6ccA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ability to print summary\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huVL3YzE_W2K",
        "colab_type": "text"
      },
      "source": [
        "**How do we arrive at the number of parameters?**\n",
        "* Our first layer has 784 output parameters (28 x 28). Each of those outputs get passed to each of the nodes in the next layer (784 x 128). We also have a bias term for each node (128) giving us 100,480 learnable parameters.\n",
        "\n",
        "> *Its worth reinforcing that, in a dense layer, every output value gets passed to every input node in the next layer. The thing that is different is the weight applied to that value, which will be different - or has the potential to be different given that this is a learned parameter. Therefore, in this instance, each node in the hidden layer will receive 784 values from the previous layer, which will have been modified by a learned weight parameter. To this, we will also add one bias term, making 785 learnable parameters.*\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "* Take the time to be comfortable with howe arrived at 1290 parameters for the output layer. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgwZkACDr4PY",
        "colab_type": "text"
      },
      "source": [
        "**So, what did we cover in this section?**\n",
        "* The building blocks of deep learning models: layers (size, shape, type) and activation functions. \n",
        "* How to build a model using the **` keras sequential API`**, specifically using a list\n",
        "\n",
        "**How does it add to our existing knowledge?**\n",
        "* This starts to add to the high level concepts we introduced in section 3.\n",
        "\n",
        "**What else can I learn to improve my knowledge?**\n",
        "* *Advanced Notebook 2: Models and Layers* covers model building with the **`functional API`** and using **`subclassing`**. It also demonstrates the output of different layer types and activations functions. \n",
        "* We will cover other ways of creating a **`sequential`** model in the following notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KknUS21-7rpc",
        "colab_type": "text"
      },
      "source": [
        "## 6. Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm3byVbvKEya",
        "colab_type": "text"
      },
      "source": [
        "**How do we train a model?**\n",
        "* As per the chart in section 3, we training by making a **`forward pass`** and a **`backward pass`**.\n",
        "* A **`forward pass`** means passing the inputs through the model and performing mathematical operations on the data as we defined in the model section to make predictions. \n",
        "* A **`backward pass`** means updating the weights and biases of our model based on the results of the predictions.\n",
        "* To do this using **`tensorflow 2.0`**,  we will use the **`keras API`**. We first pass a **`.compile()`** method to our model, and then a **`.fit()`** method to our model.\n",
        "\n",
        "**What is compiling?**\n",
        "* Compiling means that we assign certain variables to our model that are important in the training process:\n",
        "  * a **`loss function`**, which measures the distance between our predicted and actual classes. \n",
        "  * an **`optimizier`**, which updates the weights based on the backpropogation algorithm\n",
        "  * a **`metric`** to provide an appropriate additional measure of training performance.  and an accuracy \n",
        "* This is not an exhaustive list of what we variables we can pass to the **`.compile()`** method. We will pass additional variables in the following notebooks. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOlp_i1K8hU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can see that this is a relatively easy thing to code\n",
        "\n",
        "# we pass in the optimiser that we wish to use \n",
        "model.compile(optimizer='adam',\n",
        "             # specify the loss function \n",
        "             loss='sparse_categorical_crossentropy',\n",
        "             # and we specify our metrics  \n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmM9RMNts02S",
        "colab_type": "text"
      },
      "source": [
        "**What is fitting?**\n",
        "* The **`.fit()`** method governs the training process. To fit our model to the data is to pass the data through the model and see how well it predicts what we want it to predict, and then to update the model based on these results - that is, **`.fit()`** implements the forward and backward pass based on the model and training variables we have defined.\n",
        "* In the **`.fit()`** method, we get to define the number of **`epochs`** (the number of times we go through the data), and a number of other variables that we will cover in following notebooks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0cK6I1R8yfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we just pass the fit method to the model, along with training data and corresponding output data \n",
        "# We also specify the number of epochs  \n",
        "history = model.fit(train_images,train_labels,epochs=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL8vb54ktBhs",
        "colab_type": "text"
      },
      "source": [
        "**What do the different parts of the training print out mean?**\n",
        "* The left hand side shows the number of epochs we have performed. \n",
        "* For each epoch, we can see thow much of the training data we have used (its now 60000 / 60000 because training is over, but this figure changes as the data is loaded and passed through the model.)\n",
        "* On the right hand side of the data, we can see: the time it has taken to complete the epoch; the loss for that epochm abd the accuracy for that epoch.\n",
        "\n",
        "**How did we do?**\n",
        "* On the face of it, 89% accuracy seems pretty good. Given this is a publicly available dataset, then we can search to see how others have done on it. We will also apply different models to this dataset to see if we can improve our accuracy. \n",
        "* We can see that our loss steadily reduced as the epochs increased, and the accuracy improved. This is a good sign and we would probably see further improvement if we ran the training for more epochs.\n",
        "* The **`keras .fit()`** method returns a **`history`** object, which records the values of the training. We will explore this now and use it to easily plot these values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hNGFuXugw2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit returns a history object, which contains a history dictionary about everything that happened during training \n",
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSlZCVNTzHx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for example we can access the loss like this\n",
        "history_dict['loss']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Caum0GE8_Ktk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## we can pass this to a pandas dataframe\n",
        "\n",
        "# we need to import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# pass history data to dataframe object\n",
        "history_df = pd.DataFrame(history_dict)\n",
        "\n",
        "# and display it \n",
        "history_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exPpYYWOzNo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can use plot functionality of pandas to quickly plot our results\n",
        "history_df.plot(figsize=(8,5))\n",
        "# tailor our plot. Show the grid\n",
        "plt.grid(True)\n",
        "# set the vertical range to [0 -1]\n",
        "plt.gca().set_ylim(0,1)\n",
        "# display plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "891fHPxXjQkF",
        "colab_type": "text"
      },
      "source": [
        "**What does this show?**\n",
        "* The chart shows the improvement in loss and accuracy values over the epochs.\n",
        "* If we had used a validation set, we would be able to compare the values from the validation and training sets at each epoch, which would tell us a little more about how the model will be able to generalise to unseen data (we will look at this in next notebook)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BqillrujJAm",
        "colab_type": "text"
      },
      "source": [
        "**So, what did we cover in this section?**\n",
        "* That we need to perform a forward and backward pass to do training in deep learning.\n",
        "* That to do this in **`tensorflow 2.0`** is fairly easy using the **`keras API`**. We simply: \n",
        "  * pass a **`.compile()`** method to our model to define our **`loss`**, **`optimiser`** and **`metric`** variables\n",
        "  * and pass the **`.fit()`** method to our model in order to set the number of training loops and govern other related behaviour of the training process. \n",
        "\n",
        "**How does it add to our existing knowledge?**\n",
        "* This starts to add to the high level concepts we introduced in section 3.\n",
        "\n",
        "**What else can I learn to improve my knowledge?**\n",
        "* We will provide more detail about **`optimizers`**, **`loss functions`** and **`metrics`** in the following notebooks. \n",
        "* We will pass additional arguments to the **`.compile()`** and **`.fit()`** methods in the following notebooks. For example, we can add a validation set to our training loop, and we can record more information about our model performance.\n",
        "* *Advanced Notebook 3: Training* covers training using custom training loops in **`tensorflow 2.0`**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hwIqC3b6mRr",
        "colab_type": "text"
      },
      "source": [
        "## 7. Evaluation and Inference "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXu4DI9_67JG",
        "colab_type": "text"
      },
      "source": [
        "**How do we know how we well the model is performing?**\n",
        "* We have seen how the model performs on the training set, but we need to test it on unseen data to see how well the model really is peforming. \n",
        "* To do this we can call the **`.evaluate()`** method on the model and pass in our test set. This returns the loss and scoring metric that we passed in to the **`.compile()`** method (in this case, accuracy). \n",
        "* We can also use the **`.predict()`** method to make predictions on the test image. This method will return (in this instance) an array of 10 values, each representing a class label and the probability the model believes the passed in image is one of the 10 classes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3HKT8NM64e5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use the evaluate method\n",
        "# it returns two values, the loss and accuracy from our model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNeLxr3y65Vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# here we can print out the accuracy and loss\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "print('\\nTest loss:', test_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK2UEiLf8Txv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use our model to make predictions\n",
        "predictions = model.predict(test_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D8qAY3v8m5b",
        "colab_type": "text"
      },
      "source": [
        "**What has been returned?**\n",
        "* a prediction for each image. The prediction actually is a value against each of the 10 lablels for each image. This value is the probability the model has given that one of the 10 labels is correct for the unseen image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6XGdG1u9liI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can view the predictions for just one of the predictions  \n",
        "predictions[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaIvo4hw9qcP",
        "colab_type": "text"
      },
      "source": [
        "**What is the predicted label?**\n",
        "* The one with the highest probability. We can retrieve this using the np.argmax function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q30VwFXu-Qne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get highest value of the predictions \n",
        "np.argmax(predictions[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjji3qai-OmQ",
        "colab_type": "text"
      },
      "source": [
        "**How can we compare this with the actual label?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYxGHzQu-pZ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can use class_names to see what was the 9th label of the classes\n",
        "class_names[np.argmax(predictions[0])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGnr1j9m-3gG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and the get our test_label\n",
        "test_labels[0] == class_names[np.argmax(predictions[0])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om1aEGNjAcLj",
        "colab_type": "text"
      },
      "source": [
        "###7.1 Plotting our results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfO_H8HBAhgE",
        "colab_type": "text"
      },
      "source": [
        "**How can we display our results?**\n",
        "* We can use **`matplotlib`** to  display the results of our model.\n",
        "* We will plot an image, what the predicted label is, and whether this was correct. We will also plot a bar chart showing the probability assigned by the model to the different classes. We start by defining some helper functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14tVecZWAnzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a function that plots the predicted image\n",
        "\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "  # assign variable names to our parameters\n",
        "  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
        "  # remove grid and axis values \n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  # display images \n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "  # return predicted label\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  # and assign it a colour based on whether it was correct\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "  # define label format  \n",
        "  plt.xlabel(\"{}{:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                     100*np.max(predictions_array),\n",
        "                                     class_names[true_label],\n",
        "                                     color=color))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0A6jUUJCnmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot a function to graph the probabilities \n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  # assign variable names to our parameters\n",
        "  predictions_array, true_label = predictions_array[i], true_label[i]\n",
        "  # remove grid and axis values \n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  # plot a bar chart\n",
        "  thisplot = plt.bar(range(10), predictions_array, color='#777777')\n",
        "  # reduce y axis to between 0,1 values \n",
        "  plt.ylim([0,1])\n",
        "  # create prediction \n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  # set plot colour\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKCKiUp6D1ER",
        "colab_type": "text"
      },
      "source": [
        "**Now let's use our functions to plot one image and a series of images together**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IipgV7xID0d_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# look at the 0 image\n",
        "i = 0\n",
        "# set size of figure for the plot\n",
        "plt.figure(figsize=(6,3))\n",
        "# display image one side of the figure\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions, test_labels, test_images)\n",
        "# display chart on the other side of the figure\n",
        "plt.subplot(1,2,2) \n",
        "plot_value_array(i, predictions, test_labels)\n",
        "# show plot\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGzthv51E4SV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# look at the 12 image\n",
        "i = 12\n",
        "# as above \n",
        "plt.figure(figsize=(6,3))\n",
        "# as above\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions, test_labels, test_images)\n",
        "# as above\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, predictions, test_labels)\n",
        "# as above \n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN1U2TuqFD9o",
        "colab_type": "text"
      },
      "source": [
        "**We can use this functionality to plot more than one image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFPez_mCqIqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the first X test images, their predicted labels, and the true labels.\n",
        "# Color correct predictions in blue and incorrect predictions in red.\n",
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_image(i, predictions, test_labels, test_images)\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_value_array(i, predictions, test_labels)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA4zJtm8O6FC",
        "colab_type": "text"
      },
      "source": [
        "### 7.2 Making a prediction on a single image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNKjKQ70Kx_O",
        "colab_type": "text"
      },
      "source": [
        "**Didn't we do this earlier?**\n",
        "* No. We made predictions on the entire test set and then selected one of those predictions to view. Here we just want to make a prediction on one of the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYjRSk6uK3RR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get an image from the test_dataset\n",
        "img = test_images[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcjsoR8OLCcQ",
        "colab_type": "text"
      },
      "source": [
        "**We need to talk about batches...**\n",
        "* We haven't encountered **`batch sizes`** yet. This a value that represents the number of images (or other data type if we aren't doing image classification) that are passed to the model. \n",
        "* In training, we pass a batch size and this represents how many images the model will make predictions on  before recording the loss and updating the model. In the example above, this value defualted to the entire dataset (60000).\n",
        "* When making predictions with test (or other) images, **`tf.keras`** needs to make predictions on a batch. This means, it needs to understand how many images are about to passed to it. In practical terms, this means we need to add a batch dimension to the shape of our data.\n",
        "* Getting the data into the right shape for modelling and predicting can be a bit tricky sometimes. The [Advanced Notebook: Tensors](link) covers this in more detail."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btdPS6rZK8uK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets looks at the shape of image before we add a batch dimension to it\n",
        "print(img.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R71TNvYqL5xi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add batch dimension by adding a dimension to the image shape\n",
        "img = (np.expand_dims(img,0)) \n",
        "print(img.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk2l7Q8pMYQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make prediction\n",
        "predictions_single = model.predict(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGYkwMYNMoSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show the array \n",
        "predictions_single"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxvELYv6M9bQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# again we can use argmax\n",
        "np.argmax(predictions_single[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRVYzAAdMrr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and we can plot the result \n",
        "plot_value_array(0, predictions_single, test_labels)\n",
        "#\n",
        "_ = plt.xticks(range(10), class_names, rotation=45)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LmoMm3RMq_q",
        "colab_type": "text"
      },
      "source": [
        "**So, what did we cover in this section?**\n",
        "* The **`.evaluate()`** and **`.predict()`** methods for understanding how our models perform on unseen data.\n",
        "* Plotting our results so that we can see how the model did\n",
        "* Predicting the class of a single image by passing in a **`batch`** dimension to the image shape. \n",
        "\n",
        "**How does it add to our existing knowledge?**\n",
        "* This starts to add to the high level concepts we introduced in section 3.\n",
        "\n",
        "**What else can I learn to improve my knowledge?**\n",
        "* We will cover batches further in notebook 2. \n",
        "* We will use the **`.evaluate()`** and **`.predict()`** methods in later notebooks too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMhDLmZK7HW2",
        "colab_type": "text"
      },
      "source": [
        "## 8. Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ-jeIV67J9U",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/DanRHowarth/Tensorflow-2.0/blob/master/Notebook%201%20-%20Summary_final.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QgSdpKSHYgZ",
        "colab_type": "text"
      },
      "source": [
        "**What have we learnt?**\n",
        "* We can see from the chart above, quite a lot.\n",
        "* We have introduced core Deep Learning concepts, explained them in high level terms, and put them in to practice by writing code.\n",
        "* If you feel like this was a lot, don't worry because it was! It is a lot to take in. Let's end with the opportunity to build and train your own model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPF0Hul7hN7j",
        "colab_type": "text"
      },
      "source": [
        "## 9. Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaptC0g8hTFU",
        "colab_type": "text"
      },
      "source": [
        "* The best way to learn code is to write it out for yourself. Take the opportunity to reinforce what you have learnt by adding code cells below and doing the following:\n",
        "\n",
        "**Fit the model**\n",
        "* Change the number of epochs when you **`.fit()`** the model again and see how performance changes.\n",
        "\n",
        "**Build a new model**\n",
        "* Create a new model and:\n",
        "  * add an additional layer; and/or:\n",
        "  * add more nodes to the hidden layer(s)\n",
        "* You will need to recompile the model, which means adding the **`.compile()`** method to the new model and passing in the arguments we set out above.\n",
        "\n",
        "**Good luck!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7UHlJayIxxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}