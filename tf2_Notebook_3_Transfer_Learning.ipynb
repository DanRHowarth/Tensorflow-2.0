{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf2_Notebook 3_Transfer Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "fZFp3Z4AYdo8",
        "8AENJrLrsGwp",
        "22ffYRXuk-V0",
        "SoJszysjhYhr"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanRHowarth/Tensorflow-2.0/blob/master/tf2_Notebook_3_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyf7BCwOYdmA",
        "colab_type": "text"
      },
      "source": [
        "# tensorflow 2.0: Notebook 3: Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZFp3Z4AYdo8",
        "colab_type": "text"
      },
      "source": [
        "##1. Introduction to Notebook 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrZ8GySbY6IJ",
        "colab_type": "text"
      },
      "source": [
        "* In the previous notebooks, we looked at deep learning concepts and computer vision, including building Convolution Neural Networks.\n",
        "* In this notebook, we will look at:\n",
        "  * the concept of **`Transfer Learning`** and how to implement it. We will look at how transfer learning lets us build state of the art model using two techniques called **`feature extraction`** and **`fine tuning`**.  \n",
        "  * Using **`checkpoints`** to monitor performance during training, and specifically to save our model during training when its performance has improved. \n",
        "* The diagram below summarises the key points we will cover in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E-7xMKr9tLu",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/DanRHowarth/Tensorflow-2.0/blob/master/Notebook%203%20-%20Summary_final.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "489SNpfgZHUW",
        "colab_type": "text"
      },
      "source": [
        "###1.1 Load Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA_teKgj-8aQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## load libraries \n",
        "\n",
        "# we need to install tensorflow 2.0 on the google cloud notebook we have opened\n",
        "!pip install -q tensorflow==2.0.0-alpha0\n",
        "\n",
        "## importing as per previous notebook\n",
        "\n",
        "# We are future proofing by importing modules that modify or replace exising modules that we may have used now \n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# import tensorflow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# import helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# let's print out the version we are using \n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd3ftZP1ZL_C",
        "colab_type": "text"
      },
      "source": [
        "###1.2 Load and split data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMz0BhFNbsMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Use TensorFlow Datasets to load the cats and dogs dataset\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eP__0AHlaRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## this notebook will feature a lot less comments \n",
        "## explaining the code. we introduce new code and it is a good \n",
        "## exercise to look up what is does \n",
        "\n",
        "#\n",
        "SPLIT_WEIGHTS = (8, 1, 1)\n",
        "#\n",
        "splits = tfds.Split.TRAIN.subsplit(weighted = SPLIT_WEIGHTS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnx5wCc2qCAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load returns a dataset object and associated methods\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
        "    #\n",
        "    'cats_vs_dogs', split=list(splits),\n",
        "    #\n",
        "    with_info = True, as_supervised = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-vG65VHqtsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the dataset objects return (image, label) pairs\n",
        "print(raw_train)\n",
        "print(raw_validation)\n",
        "print(raw_test)\n",
        "\n",
        "## how many channels do the images have?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwbZuTuIrdqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "get_label_name = metadata.features['label'].int2str"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAcjjpqBr7R9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "for image, label in raw_train.take(2):\n",
        "  #\n",
        "  plt.figure()\n",
        "  #\n",
        "  plt.imshow(image)\n",
        "  #\n",
        "  plt.title(get_label_name(label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY4us-KBsTmt",
        "colab_type": "text"
      },
      "source": [
        "###1.3 Format the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18XFwmh2sYTS",
        "colab_type": "text"
      },
      "source": [
        "**What sort of things do we need to do to the data?**\n",
        "* As mentioned in previous notebooks, we need to **`preprocess`** the images to get them into the same size and shape. \n",
        "\n",
        "**Specifically what will we do here?**\n",
        "* Resize images to 160 using the **`tf.cast`** method and mapping it to each set of images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnZT2CNKszAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All images will be resized to 160x160\n",
        "IMG_SIZE = 160 \n",
        "\n",
        "#\n",
        "def format_examples(image, label):\n",
        "  #\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  #\n",
        "  image = (image/127.5) - 1\n",
        "  # \n",
        "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "  #\n",
        "  return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZVeUb7It80d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply function using map -> try just doing with function \n",
        "#\n",
        "train = raw_train.map(format_examples)\n",
        "#\n",
        "validation = raw_validation.map(format_examples)\n",
        "#\n",
        "test = raw_test.map(format_examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYBqJFw3vLVn",
        "colab_type": "text"
      },
      "source": [
        "###1.4 Shuffling and Batching \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOT9zKs0BbN_",
        "colab_type": "text"
      },
      "source": [
        "**What is shuffling and batching?**\n",
        "* We discussed **`batching`** in the previous notebook. Batch size is specified in the code below, and is a different approach than in the previous notebook when we specified batch size in the **`.fit()`** method.\n",
        "* **`Shuffling`** reorders the data samples as they are passed to the model, and can be used to ensure the model doesn't see the same sequence of data during each epoch of training, which may influence how the model learns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_flJPrsivFtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shuffle and batch the data \n",
        "#\n",
        "BATCH_SIZE = 32\n",
        "SHUFFLE_BUFFER_SIZE = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkfmEJAQve2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "#\n",
        "validation_batches = validation.batch(BATCH_SIZE)\n",
        "#\n",
        "test_batches = test.batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlWOfFGGw9uh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's look at the data\n",
        "#\n",
        "for image_batch, label_batch in train_batches.take(1):\n",
        "  #\n",
        "  pass \n",
        "#\n",
        "image_batch.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JZPZmWgdOt0",
        "colab_type": "text"
      },
      "source": [
        "**So, what have we covered?**\n",
        "* We have looked at loading libraries, loading and splitting the data, formatting the data, and shuffling and batching. \n",
        "\n",
        "**How does it fit in with what we have covered previously?**\n",
        "* We have loaded data and batched it before, but this has been done in a different way in this notebook.\n",
        "* Shuffling is new and adds to our knowledge about passing data to the model \n",
        "\n",
        "**What else can I learn to improve my knowledge?**\n",
        "* We have not augmented our data, which is another way to pass data to our model. In augmentation, an image is changed slightly so that the model doesn't see the same image twice. This makes models robust and means we can train with less data. \n",
        "* For more on Data Augmentation see *Advanced Notebook 3*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peu_D-JXg9PJ",
        "colab_type": "text"
      },
      "source": [
        "##2. Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKV5aGjREkP6",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/DanRHowarth/Tensorflow-2.0/blob/master/Notebook%203%20-%20ImageNet%20Scores_final.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v3YtPu1hChE",
        "colab_type": "text"
      },
      "source": [
        "**What is transfer learning?**\n",
        "* The image above ([source](https://www.researchgate.net/figure/Winner-results-of-the-ImageNet-large-scale-visual-recognition-challenge-LSVRC-of-the_fig7_324476862) with my annotations) shows the progress made in tackling what was until recently the benchmark computer vision challenge.\n",
        "* We can see the impact made by deep learning. These models are developed by top research bodies and companies, and trained for a long period of time.\n",
        "* These models are available to use via transfer learning, which allows us to load a previously trained model (in a way, as we did when we saved and loaded our trained model) and use it for purposes. \n",
        "\n",
        "**What is transferred?**\n",
        "* The weights and biases - the training parameters - of the model.\n",
        "\n",
        "**So we just use the same model?**\n",
        "* We do download the model as-is, so both its weights and biases and its architecture (which house the weights and biases)\n",
        "* But because a model is trained for one purpose (in the example above, on the imagenet data set), we need to repurpose the model for our own needs. In reality, this means one of two things:\n",
        "  * keeping the convolutional base and creating a new classifier layer. We can see that this would be required if our dataset has less output classes than the dataset trained on by the original layer. But we can also think about this as remapping the information extracated by the model and contained in the final output of the convolutional base to the new output classes. This is known as **`feature extraction`**. Because we have a new classifier (or 'head'), we will train this part of the model. \n",
        "  * We can also choose, once we have done feature extraction, to retrain some of the top layers of the convolutional base in order that the information represented by these layers (weights and biases) is more tailored to our new dataset.\n",
        "* We will look at both of these techniques in this notebook.\n",
        "* The diagram below uses the VGG archictecture to explain the difference between the two approaches [source](https://www.researchgate.net/figure/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means_fig2_325137356). \n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNH_T0jG7FU9",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/DanRHowarth/Tensorflow-2.0/blob/master/Notebook%203%20-%20Types%20of%20Transfer%20Learning_final.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awLKQ1AfjQAn",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Glvd3vzakMwl",
        "colab_type": "text"
      },
      "source": [
        "**What is Feature Extraction?**\n",
        "* We use the representations learned by a previous network to extract meaningful features from new samples. This means we use all the weights in the convolutional base and add a new classifier (dense layer) on top. \n",
        "\n",
        "**How does this work?**\n",
        "* The model has already learned representations in the conv layers that are common to lots of images. We use those in our model.\n",
        "\n",
        "**Why do we need a new classifier layer?**\n",
        "* Our output class size is likely to be a different size from the one the model is trained on. \n",
        "* We need to map, or remap, the representations of the convolutional base to the output layer, which requires relearning what sort of representations relate to what sort of output.\n",
        "\n",
        "**What's the process for doing this?**\n",
        "* We will go through this, but the main steps are:\n",
        "  * get our model\n",
        "  * freeze the base model\n",
        "  * add a classification layers ('head')\n",
        "  * train the classification layers\n",
        "\n",
        "**How do we train?**\n",
        "* All the layers in our convolutional base are frozen. So the resulting final set of feature maps will be a product of the original dataset and the learned weights. These weights are not updated.\n",
        "* They are then passed to the classifier layer and mapped to the output values, which are then trained as normal. \n",
        "\n",
        "\n",
        "**How do we build a new classifier?**\n",
        "* As we did when we built a classifier in the previous notebooks. \n",
        "\n",
        "**What are we using?**\n",
        "* **`MobileNet v2`**, pretrained on ImageNet dataset, 1.4M images and 1000 classes.\n",
        "* We won't cover much on the MobileNet v2 architecture is, but the relevant paper that introduces this model is [here](https://arxiv.org/pdf/1801.04381.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nAo6vhdxSnr",
        "colab_type": "text"
      },
      "source": [
        "#### STEP 1: GET MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g__QbBXjyEom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## instatiate model preloaded with weights  \n",
        "## a good exercise would be to ensure you understand the parameters\n",
        "\n",
        "#\n",
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "# create base model\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape = IMG_SHAPE,\n",
        "                                              # \n",
        "                                              include_top = False,\n",
        "                                              #\n",
        "                                              weights = 'imagenet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnebO9egjpZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this changes the image shape to bottleneck layer shape (our top layer of the base model)\n",
        "#\n",
        "feature_batch = base_model(image_batch)\n",
        "# \n",
        "print(feature_batch.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk9r99cb2g40",
        "colab_type": "text"
      },
      "source": [
        "#### STEP 2: FREEZE BASE MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNezr1cF3BvP",
        "colab_type": "text"
      },
      "source": [
        "* Freeze model before compiling and training \n",
        "* Prevents weights in a given layer being updated during training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtBIQyuY29I1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "base_model.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOqr_9do3ggP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# look at base model architecture\n",
        "base_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCkqpg8CcW1r",
        "colab_type": "text"
      },
      "source": [
        "* Hopefully some of the layers will seem familiar to you. We have in no way covered them,\n",
        "* Of the ones you see in thte summary, it is worth understanding more about [batch normalisation](https://arxiv.org/abs/1502.03167) as this has become an important layer in building effective models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUva8Yf13lzH",
        "colab_type": "text"
      },
      "source": [
        "#### STEP 3: ADD CLASSIFICATION LAYER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIpjhX5Q38z0",
        "colab_type": "text"
      },
      "source": [
        "* Convert bottleneck layer. (A bottleneck layer is another for the flattened array from the convolutional base, prior to it being passed to the classification layer.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLs7cpfj5QY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# average over the 5 x 5 spatial locations using tf.keras.layers.GlobalAveragePooling2D\n",
        "#\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
        "#\n",
        "feature_batch_average = global_average_layer(feature_batch)   \n",
        "#\n",
        "print(feature_batch_average.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcBXiTvK6RbQ",
        "colab_type": "text"
      },
      "source": [
        "**What just happened?**\n",
        "* We can see passing our data to the convolutional base returned 1280 feature maps of 5 x 5 shape.\n",
        "* We now have a 1D array of 1280. Look up `GlobalAveragePooling2D` to see how this has happened."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqDXVTMSjrlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add new classifier \n",
        "# use tf.keras.layers.Dense\n",
        "#\n",
        "prediction_layer = keras.layers.Dense(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsxreZQ37Abv",
        "colab_type": "text"
      },
      "source": [
        "**How does this work as a classification layer?**\n",
        "* We have created a binary classification layer. We don't need activation, we just want to map the 1280 values to one value, either 1 or 0. This predicition is treated as a **logit**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HnpfcKB7VDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "prediction_batch = prediction_layer(feature_batch_average)\n",
        "#\n",
        "print(prediction_batch.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZdgS4Mg8sm5",
        "colab_type": "text"
      },
      "source": [
        "**How do we turn these layers into a model?**\n",
        "* Using the Sequential API, we can pass these layers in as a list\n",
        "\n",
        "**We can pass a model into another model?**\n",
        "* Yes. Our first 'layer' is a model. Think about what this **`base_model`** returns - an output that can be taken in by another layer and modelled by that layer. In this sense, models are just like layers and are treated as such by tensorflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIvH9Soc7jUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use a list...\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    global_average_layer,\n",
        "    prediction_layer\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjVnmHxZ_0W0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l12cj7W9iG7",
        "colab_type": "text"
      },
      "source": [
        "#### STEP3: TRAINING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZceTmpnNksen",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## we tend to use a smaller learning rate when doing feature extraction\n",
        "\n",
        "base_learning_rate = 0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piN-NlGE-mUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n",
        "             #\n",
        "             loss = 'binary_crossentropy',\n",
        "             #\n",
        "             metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJatntLuAmkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "num_train, num_val, num_test = (\n",
        "  #\n",
        "  metadata.splits['train'].num_examples * weight / 10\n",
        "  for weight in SPLIT_WEIGHTS\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eb1kh15A-3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "initial_epochs = 2      ## training can be slow on this dataset. Set a low epoch\n",
        "                        ## number at first so that you can complete the training loop\n",
        "                        ## and return a history object."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9L-DdLKB-_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "history = model.fit(train_batches,\n",
        "                   #\n",
        "                   epochs = initial_epochs,\n",
        "                   #\n",
        "                   validation_data = validation_batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usxwFy7hk1J-",
        "colab_type": "text"
      },
      "source": [
        "**How did we do?**\n",
        "* What accuracy did we get?\n",
        "* Is this any good?\n",
        "* Let's take a look at our learning curves to visualise our performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B2nf1jidshN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "history.history.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bRUnZeJkzJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "acc = history.history['accuracy']\n",
        "#\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "#\n",
        "loss = history.history['loss']\n",
        "#\n",
        "val_loss = history.history['val_loss']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDfmOR8dC95o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "plt.figure(figsize=(8,8))\n",
        "#\n",
        "plt.subplot(2, 1, 1)\n",
        "#\n",
        "plt.plot(acc, label = 'Training Accuracy')\n",
        "#\n",
        "plt.plot(val_acc, label = 'Validation Accuracy')\n",
        "# \n",
        "plt.legend(loc = 'lower right')\n",
        "# \n",
        "plt.ylabel('Accuracy')\n",
        "#\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "#\n",
        "plt.title('Training and Validation Accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sjqYvbAEpWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "plt.subplot(2, 1, 1)\n",
        "#\n",
        "plt.plot(loss, label = 'Training Loss')\n",
        "#\n",
        "plt.plot(val_loss, label = 'Validation Loss')\n",
        "# \n",
        "plt.legend(loc = 'upper right')\n",
        "# \n",
        "plt.ylabel('Cross Entropy')\n",
        "#\n",
        "plt.ylim([0, 1.0])\n",
        "#\n",
        "plt.title('Training and Validation Loss')\n",
        "#\n",
        "plt.xlabel('epoch')\n",
        "#\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxM7u1ZRkxat",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Fine Tuning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SroEBrr9D4AX",
        "colab_type": "text"
      },
      "source": [
        "**What is fine tuning?**\n",
        "* We fine tune the top layers of the Convolutional base. We do this *after* we have trained a classifier as per FE\n",
        "\n",
        "**Why?**\n",
        "* Because we think we can gain more accuracy from having the top layers of the model base be trained on the actual images. By extension, this means that they will be more tailored to our image set and less generic. \n",
        "\n",
        "**What is the process?**\n",
        "* Go through the feature extraction step as above\n",
        "* Unfreeze final layers for training. Train model. \n",
        "\n",
        "**Why do we need to train the classifier first?**\n",
        "* If you add a randomly initialized classifier on top of a pre-trained model and attempt to train all layers jointly, the magnitude of the gradient updates will be too large (due to the random weights from the classifier) and your pre-trained model will forget what it has learnt. \n",
        "\n",
        "**Why only unfreeze final layers?**\n",
        "* Early layers have learned general features that we can use. If we unfreeze the earlier we may as well train a model from scratch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbrluFJ619Zn",
        "colab_type": "text"
      },
      "source": [
        "#### STEP 1: UNFREEEZE TOP LAYERS OF MODEL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdpTVMHzlgOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## unfreeze layers\n",
        "\n",
        "# \n",
        "base_model.trainable = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iIjkgDJ2eG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# look at how many layers there are in the base model\n",
        "print(\"Number of layers in the base model:\", len(base_model.layers))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1uMH5_h2sOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fine tune from this layer onwards\n",
        "fine_tune_at = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WS8dYmO2xrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze all layers before this layer\n",
        "#\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "  layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsydSZ1t3e3m",
        "colab_type": "text"
      },
      "source": [
        "#### STEP 2: RECOMPILE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxZoVNztljNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n",
        "             #\n",
        "             loss = 'binary_crossentropy',\n",
        "             #\n",
        "             metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUw9D8OV3rFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYCdkC3O4N5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "fine_tune_epochs = 2        ## again, this should be more but training might be slow\n",
        "                            ## so see if we can train 2 epochs first\n",
        "#\n",
        "total_epochs = initial_epochs + fine_tune_epochs "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPrj4fSiEI3S",
        "colab_type": "text"
      },
      "source": [
        "**Saving the model**\n",
        "* Let's add an additional element to the training loop -  a callback\n",
        "\n",
        "**Whats a callback?**\n",
        "* A callback access the training data and allows us to apply functions to it. In this instance we want to monitor the training data and saves the weights of the model at a certain point.\n",
        "\n",
        "\n",
        "**How do we do it?**\n",
        "* As below, using the `tf.keras.callbacks.ModelCheckpoint` class to create the callback that will save our model, and pass it to the `model.fit() `metthod.\n",
        "\n",
        "**Will we be able to save this to the cloud (Google Drive)?**\n",
        "* With this code, no. We will update the code so that it does work. For now, it is just worth seeing the code and understanding that saving our models during training is an option. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYCMwTQaFD3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## create a checkpoint callback\n",
        "# \n",
        "checkpoint_path = 'training_1/cp-{epoch:04d}.ckpt'  \n",
        "#\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ol8ImspFHMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                 #\n",
        "                                                 save_weights_only = True,\n",
        "                                                 #\n",
        "                                                 verbose = 1,\n",
        "                                                 #\n",
        "                                                 period = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYUwk_go36qN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "history_fine = model.fit(train_batches,\n",
        "                         #\n",
        "                         epochs = total_epochs,\n",
        "                         # resumes training at our stopping point\n",
        "                         initial_epoch = initial_epochs,\n",
        "                         #\n",
        "                         validation_data = validation_batches,\n",
        "                         #\n",
        "                         callbacks = [cp_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlGzk1vNlkeX",
        "colab_type": "text"
      },
      "source": [
        "**How did we do?**\n",
        "* What do you think?\n",
        "* \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bZutJYgGcpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## redundant code for this notebook as we have not saved a checkpoint directory on the drive\n",
        "\n",
        "# list our cp directory\n",
        "# !ls {checkpoint_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNe5bltYlqL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "acc += history_fine.history['accuracy']\n",
        "#\n",
        "val_acc += history_fine.history['val_accuracy']\n",
        "\n",
        "#\n",
        "loss += history_fine.history['loss']\n",
        "#\n",
        "val_loss += history_fine.history['val_loss']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRpCDNrv5Hp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# these could be a function i think\n",
        "#\n",
        "plt.figure(figsize=(8,8))\n",
        "#\n",
        "plt.subplot(2, 1, 1)\n",
        "#\n",
        "plt.plot(acc, label = 'Training Accuracy')\n",
        "#\n",
        "plt.plot(val_acc, label = 'Validation Accuracy')\n",
        "#\n",
        "plt.ylim([0.8, 1])\n",
        "#\n",
        "plt.plot([initial_epochs -1, initial_epochs -1],\n",
        "        plt.ylim(), label = 'Start Fine Tuning')\n",
        "# \n",
        "plt.legend(loc = 'lower right')\n",
        "#\n",
        "plt.title('Training and Validation Accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5K-txfR5u2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# these could be a function i think\n",
        "#\n",
        "plt.subplot(1, 1, 1)\n",
        "#\n",
        "plt.plot(loss, label = 'Training Loss')\n",
        "#\n",
        "plt.plot(val_loss, label = 'Validation Loss')\n",
        "#\n",
        "plt.ylim([0, 1.0])\n",
        "#\n",
        "plt.plot([initial_epochs -1, initial_epochs -1],\n",
        "        plt.ylim(), label = 'Start Fine Tuning')\n",
        "# \n",
        "plt.legend(loc = 'upper right')\n",
        "#\n",
        "plt.title('Training and Validation Loss')\n",
        "#\n",
        "plt.xlabel('epoch')\n",
        "#\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxWh3c72lxOA",
        "colab_type": "text"
      },
      "source": [
        "**So, what have we have covered?**\n",
        "* Feature Extraction and fine tuning pre-trained convolutional models\n",
        "\n",
        "**How does it fit in with what we have covered previously?**\n",
        "* We built a standard CNN earlier, this now provides us with state of the art models.\n",
        "\n",
        "**What else can I do to further my knowledge?**\n",
        "* Look at other models that can be downloaded and used via transfer learning \n",
        "* Look at the other callbacks that are available during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AENJrLrsGwp",
        "colab_type": "text"
      },
      "source": [
        "## 3. INFERENCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLlyYvsTFPvw",
        "colab_type": "text"
      },
      "source": [
        "* If we had saved a model, we could load it now and use it for inteference.\n",
        "* For now, the code to do that has been left in \n",
        "* We will update the code shortly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5VwJ1NtlyKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to load we first create a new instance of our model\n",
        "# new_model = # new instance of model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIC2QYOFsqhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# new_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gS0UwjhG7IM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# then we can load the weights\n",
        "# new_model.load_weights(checkpoint_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz1Oy6ATk4Tv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we can perform inference as we did in previous notebooks\n",
        "model...."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ffYRXuk-V0",
        "colab_type": "text"
      },
      "source": [
        "##4. Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHs7qL1FlA99",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/DanRHowarth/Tensorflow-2.0/blob/master/Notebook%203%20-%20Deep%20Learning%20Concepts%20with%20content.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq-WmQf3lfuR",
        "colab_type": "text"
      },
      "source": [
        "* The chart above sets out the main things we have covered in the last three notebooks. That's quite a lot!\n",
        "* Don't worry if you don't understand it all. Hopefully the framework will help you piece it all together, but remember to try other tutorials and go over the same topics a few times until you start to understand them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoJszysjhYhr",
        "colab_type": "text"
      },
      "source": [
        "##5. Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP58xfWGha-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## load a different model from tf.keras.applications\n",
        "## perform feature extraction. create a different classifier than the one we used, perhaps add another layer\n",
        "## then perform fine tuning\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}