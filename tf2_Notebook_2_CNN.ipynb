{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf2_Notebook 2_CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "QS8ejf4q1Af6",
        "4q2volUNyV8i",
        "w-wZF5jB6YMu",
        "KqT0LadcXjut"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanRHowarth/Tensorflow-2.0/blob/master/tf2_Notebook_2_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uxgRHhoyKXR",
        "colab_type": "text"
      },
      "source": [
        "# tensorflow 2.0: Notebook 2: Computer Vision with CNNs "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS8ejf4q1Af6",
        "colab_type": "text"
      },
      "source": [
        "## 1. Introduction to this Notebook "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an-J0lbeYKJx",
        "colab_type": "text"
      },
      "source": [
        "* In the previous notebook we introduced deep learning concepts and the tensorflow 2.0 code to implement these concepts.\n",
        "* In this notebook, we will use another dataset - the **`mnist`** dataset -  to build on our knowledge. In particular, we will:\n",
        "  * introduce **`Computer Vision`** \n",
        "  * introduce **`convolutional layers`** into our models \n",
        "  * introduce the concept of **`regularisation`** and implement **`dropout layers`** in our models\n",
        "  * introduce the **`validation set`** in training our model \n",
        "  * introduce how to **`save`** and reuse our model \n",
        "* The image below sets out how this fits within our deep learning framework and exising knowledge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQQ2UeuUnpb3",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/DanRHowarth/Tensorflow-2.0/blob/master/Summary%20of%20Notebook%202%20Concepts.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE6VXlttqngA",
        "colab_type": "text"
      },
      "source": [
        "### 1.1  Load Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2len8Zw2OWh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "ac94a84a-e0d5-4454-95fb-6059f00a8502"
      },
      "source": [
        "# we need to install tensorflow 2.0 on the google cloud notebook we have opened\n",
        "!pip install -q tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 79.9MB 309kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 27.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 419kB 39.6MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAeFpNkuq3Sd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b25ae7da-ff17-45de-e305-e65039e66ae8"
      },
      "source": [
        "## importing as per previous notebook\n",
        "\n",
        "# We are future proofing by importing modules that modify or replace exising modules that we may have used now \n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# import tensorflow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# import helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# let's print out the version we are using \n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-alpha0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLCYkpZf1-sa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## some additional imports for this notebook \n",
        "from tensorflow.keras import datasets, layers, models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy7-uMr30gxw",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Loading our Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-i1MWLw2br1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8fb90c05-9516-4de7-e57c-b61f184bd61d"
      },
      "source": [
        "# split our data \n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VozmgZb26Qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets have a quick look at our data \n",
        "plt.imshow(train_images[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8m3va3c3oOx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db9f1f4d-2905-40b2-9def-3fd892df3f76"
      },
      "source": [
        "# and at our labels\n",
        "np.unique(train_labels)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9Y29leh3hj5",
        "colab_type": "text"
      },
      "source": [
        "**What is the problem we are trying to solve?**\n",
        "* As we can see, we have images of digits from 0 - 9, and labels from 0 - 9. We are trying to build a model that correctly classifies the digits in the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q2volUNyV8i",
        "colab_type": "text"
      },
      "source": [
        "## 2. Data: Introduction to Computer Vision "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIw5BMMfYGEl",
        "colab_type": "text"
      },
      "source": [
        "**What is Computer Vision?**\n",
        "* Computer Vision is the field of how computers can gain understanding from images and videos. It includes tasks such as **`image recognition`** and **`object detection`**. Deep Learning is seen as the state of the art technology for solving computer vision problems. \n",
        "\n",
        "**Why is Deep Learning particularly good at it?**\n",
        "* The layers within a deep learning model are good for identifying and modelling the different aspects of an image (such as edges, parts of faces, and other important parts of an image). The meaning that each layer extracts can be built up to form representations for lots of different image types that can then be classified.  \n",
        "* In particular, **`convolutional layers`** are good at extracting representation from image data and they form the basis of deep learning models for image recognition. The ability to build larger and larger models that consist of these convolutional layers, and to train them with more and more data (thanks to increasing compute power), led to a leap forward in state of the art for computer vision. \n",
        "\n",
        "**How does it work?**\n",
        "* Every image is represented by an array of numbers. You may have noticed this when we looked at the **`shape`** of the images we were processing. This shape represents the number of pixels in an image, and each pixel has a numerical value. This numerical value maps to a colour value that is displayed. It is also what we use as input values to our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os-EKjfMLa9K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f1439f3-c269-42e3-d826-ec89f10df664"
      },
      "source": [
        "## lets start by looking at the shape of an image\n",
        "\n",
        "## we can see that it is 28 x 28 pixels\n",
        "train_images[0].shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if3gXQQbu8JV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## we can also see that these pixels are represented in an array of numbers \n",
        "train_images[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AOm9r_-5N6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we need plt.imshow() - or another library such as OpenCV or PIL - to output an image from this array\n",
        "plt.imshow(train_images[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDMsDeU_zACw",
        "colab_type": "text"
      },
      "source": [
        "**What do the array values mean?**\n",
        "* Each value leads to a colour for the pixel that the array value represents. Actualy what colour is displayed depends somewhat on the number of colour channels the array has. We have only one channel present in this dataset. This is grayscale channel. Typically, we will see three channels for colour images, with each channel representing one of Red, Green, Blue. A value in one channel will display a different colour than a value in another channel. \n",
        "* See the tutorials [here](https://www.w3schools.com/colors/default.asp) for more detail on how the values within a channel map to a colour.\n",
        "* Its worth noting here that there are typically 256 values (0 - 255) available in each channel, making a total combination of c. 16.8m colours available per a three channel image!\n",
        "* As per the previous notebook, we will rescale the arrays to between 0 and 1. This needs to happen in order to maximise the success of the training. \n",
        "\n",
        "**What about images of different shapes?**\n",
        "* The size of an image can and does vary. In this case, we have small image of 28 x 28 pixels (or 28, 28, 1) given we have one channel. This was the same for the previous dataset and it makes it easy to train models. \n",
        "* Outside of introductory tutorials, It is likely that you will see much larger images, meaning many more pixels and therefore larger arrays to train and learn representations on. This will make the models larger and training more involved. \n",
        "* One final thing to note is that Deep Learning models always require an array of the same size to be passed to it. This means that images which differ in size need to be preprocessed so that they are the same size before being passed to the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAxv79drLfaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we now need to reshape the data to add a colour channel \n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt4aS5nn3mpL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54b31bb0-bbd1-4bbc-cadf-b0b1d67b5046"
      },
      "source": [
        "# we can view the new shape \n",
        "train_images.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klwaHSVyL4jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and normalize the data \n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuwQA6fTUKca",
        "colab_type": "text"
      },
      "source": [
        "**So, what did we cover in this section?**\n",
        "* An introduction to computer vision, including how images are represented by arrays.\n",
        "* How the shape of an array matters for our model and the preprocessing required prior to feeding the arrays to our model.\n",
        "\n",
        "**How does it add to our existing knowledge?**\n",
        "* This builds on the deep learning concepts from notebook 2.\n",
        "\n",
        "**What else can I learn to improve my knowledge?**\n",
        "* Images have to be fed into the model in the same shape each time. This requires pre-processing.\n",
        "* Prior to feed images into a model, we can also change the image in certain ways to add noise and variety to the training data. This should mean that the model is more robust and better at generalizing to unseen data. We will look at both of these in the *Advanced: Data Augmentation* notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr6NFGzV33c7",
        "colab_type": "text"
      },
      "source": [
        "## 3. Model Building "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in1xg1Eh-iF_",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Convolutional Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plOECBsyXtCl",
        "colab_type": "text"
      },
      "source": [
        "**What did we do in the previous notebook?**\n",
        "* In [Notebook 1](link), we looked at the main elements of deep learning models: input and output layers, hidden layers - which contain the learned parameters of the model, and activation functions. \n",
        "* We also looked briefly at the different sort of hidden layers available to us, such as dense and convolutional layers. \n",
        "* And, we built model that took an image as an input and flattened it to a **1D** array. \n",
        "\n",
        "**How do we build a convolutional neural network?**\n",
        "* A convolutional neural network (CNN) contains both dense and convolutional layers. The convolutional layers form the **`base`** of the model and extracts representation from the image. The dense layers form the **`head`** of the model and takes this represetation and maps it to our output classes\n",
        "* A convolutional layer takes our image as it (subject to any preprocessing to get it in a standard shape or augmented to add noise and variety to the dataset) - that is, we do not need to flatten the image into a **`1D`** array. We flatten the array after our final convolutional layer and prior to passing our input to the dense layer.\n",
        "\n",
        "**Why use a convolutional layer?**\n",
        "* A convolution better encodes the key information in an image than other types of layers. Their application to computer vision resulted in a marked improvement in what was state of the art. That's why we use them. \n",
        "\n",
        "**What is a convolutional layer?**\n",
        "* Simply, a convolutional layer is a layer that performs a mathematical operations known as convolutional on the input data.  In contrast, a dense layer perfornms matrix multiplicaiton on its inputs. \n",
        "* Each convolutional layer have a user-defined set of filters (or windows) that we pass over the image. We define the number and size of filters, although they are typically a 3 x 3 matrix. \n",
        "* This filter contains a set of weights that will be learned by the model and which are used to multiply the input values and return a new value in the layer's output. Its these filters that contain the learning of the convolutional layers of the model, whose weights will be updated as we train so that they are more and more able to extract key information from the image.\n",
        "* The filter is applied to all the image channels as it passes over each pixel location such that it will look at a specific row and column index position and all the array values available at that index:\n",
        "$$(row, column, :)$$\n",
        "* We won't go in to *how* convolutional works here, but see the cell at the end of this section for links that do explain how it works. \n",
        "\n",
        "**So what does a convolutional layer return?**\n",
        "* A convolutional layer returns an output array of the same (row, column) shape as the input array, but with one channel only. \n",
        "* It tends to be the case that convolutional layer is paired with a **`pooling layer`**. We won't cover these in any detail, but its sufficient to know that a pooling layer tries to extract the key information from the convolutional layer while typically halving its size. \n",
        "* The diagram below set this out. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuADaW8uNb-D",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/DanRHowarth/Tensorflow-2.0/blob/master/Notebook%202%20Convolution%20and%20Pooling%20Layers.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49o6Ct5GNceO",
        "colab_type": "text"
      },
      "source": [
        "**How does these convolutional layers combine together?**\n",
        "* This describes just one convolutional layer. A state of the art model has dozens of layers. So as our input is passed through the layers, more and more feature maps are created and more pooling is done.\n",
        "* This makes the input array longer but wider??\n",
        "* Conventions on how we add filters as the model builds\n",
        "* And means we flatten it\n",
        "* *add image of overall conv layer*\n",
        "\n",
        "* **Add in the terminology here of that the filters and outputs are typically called - see FC CV section**\n",
        "* **How does convolution work on the non-image layer - i.e do we take array (row, col, 28) and it become (row, col, 1).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7oAv7C13_eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## lets build our convolutional base\n",
        "## we use the Sequential API but use .add() rather than passing the layers in as a list\n",
        "\n",
        "# build model using sequential -> use .add not list to show difference \n",
        "model = models.Sequential() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCb5IdENjxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# start adding layers. input shape has been defined, including the channel value we added via reshape earlier\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "# max pooling layers\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "# this is then repeated to build \n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "# max pooling layers\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "# additional convolutional layer \n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhezngEi5yQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print model \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7VAnSx54L-B",
        "colab_type": "text"
      },
      "source": [
        "**How do we get to the parameter count?**\n",
        "* The parameters of a convolutional layer are defined by:\n",
        "$$((filter\\ height\\ \\times filter\\ width)\\ +\\ bias\\ term)\\ \\times number\\ of\\ filters$$\n",
        "\n",
        "* The bias term is a value of 1 so the number of parameters for the first convolutional layer is:\n",
        "$$((3 \\times 3)+ 1)\\ \\times 32 = 320$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoBvH_OFUgLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# exercise with parameter count\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sly6qTF-PNXf",
        "colab_type": "text"
      },
      "source": [
        "**How do we get to the shape?**\n",
        "* Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfRNOIjiUigH",
        "colab_type": "text"
      },
      "source": [
        "**So that's it...?**\n",
        "* Need a classification layer\n",
        "* Takes the final output shape of the Conv layers. \n",
        "* Dense layers take a 1d array so we need to flatten first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKQpmbk-UjUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# flatten \n",
        "model.add(layers.Flatten())\n",
        "#\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "# \n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcUQrBgtUmEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# complete model summary\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VluCuezAJoQ5",
        "colab_type": "text"
      },
      "source": [
        "###3.1 Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWAQILZuJZnm",
        "colab_type": "text"
      },
      "source": [
        "**What is regularization?**\n",
        "* Dropout\n",
        "* Weight Decay \n",
        "* Soemthing else."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLmK_8wgUshx",
        "colab_type": "text"
      },
      "source": [
        "**So, what did we cover in this section?**\n",
        "* .add()\n",
        "\n",
        "**How does it add to our existing knowledge?**\n",
        "* Two ways of using Sequential \n",
        "\n",
        "**What else can I learn to improve my knowledge?**\n",
        "* Detail of convolution - see advanced notebook 3. Francois Chollet, Cezanne, C Olah. look this up and write it out for yourself. Convolution arxiv paper\n",
        "* different ways of specifying the conbolution \n",
        "* Other things include..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-wZF5jB6YMu",
        "colab_type": "text"
      },
      "source": [
        "## 4. Training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSEAp6bmIS9L",
        "colab_type": "text"
      },
      "source": [
        "### 4.1 Building our training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIm3yY_VKMBa",
        "colab_type": "text"
      },
      "source": [
        "**What did we do in the previous notebook/**\n",
        "* Compile with...\n",
        "* See what the differences are..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXRPgaFK6dWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets build a training loop again (with callbacks and learning rate, and validation)\n",
        "# need to add in callbacks here\n",
        "model.compile(optimizer = 'adam',\n",
        "             loss = 'sparse_categorical_crossentropy',\n",
        "             metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtZtTytW6oSf",
        "colab_type": "text"
      },
      "source": [
        "**What is different?**\n",
        "* Validation set, Callbacks, learning rate\n",
        "\n",
        "**What is validation set?**\n",
        "* Answer\n",
        "\n",
        "**What is a callback?** \n",
        "* Answer here\n",
        "\n",
        "**What is the learning rate?**\n",
        "* Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaEy1MqF6rh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# here is the training loop in action\n",
        "model.fit(train_images, train_labels, epochs = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzsUx60J7TxW",
        "colab_type": "text"
      },
      "source": [
        "**Again, what is going on with the training loop?**\n",
        "* Answer \n",
        "\n",
        "**Did we do any good?**\n",
        "* Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLvVXg9m7W75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR21IEmy7cVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# results\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPa90NroR2gU",
        "colab_type": "text"
      },
      "source": [
        "**How will we perform on the test set?**\n",
        "* Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGBqIcm8R-q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLdWxhaQIQd4",
        "colab_type": "text"
      },
      "source": [
        "###4.2 Under and Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-8Zs1myIdBp",
        "colab_type": "text"
      },
      "source": [
        "**What is this?**\n",
        "* Answer "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zIPffoBXE7x",
        "colab_type": "text"
      },
      "source": [
        "**What can we do to improve it?**\n",
        "* Regularization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I42DKC4iW3Ib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# code here showing options for changing \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5TCTvxTW_Xv",
        "colab_type": "text"
      },
      "source": [
        "**What do different learning rates do?**\n",
        "* Answer\n",
        "* Let's try different approaches "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfmBbIGV7exC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot learning rate here \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPSoYl85XWsm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# maybe more code here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO3RXhqHXe9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pull it all together here for more training and show results \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZNzgPdpP1Wq",
        "colab_type": "text"
      },
      "source": [
        "### 4.3 Saving Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocpFaHT_2wO8",
        "colab_type": "text"
      },
      "source": [
        "**What do we mean by *saving a model*?**\n",
        "* We can save our progress as we train our models. We can save our progress in two ways:\n",
        "  * *during training*, so that our models are saved after each epoch (or, after an epoch that shows model improvement). \n",
        "  * *after training*, so that our model has completed its training before we save it\n",
        "* Of course, we can opt to save the model both during *and* after training. \n",
        "* Using `tensorflow 2.0`, we can opt to save the model either manually, i.e. after the model has trained, or by using callbacks - i.e. incorporating saving into the training process. \n",
        "  \n",
        "**What are we saving?**\n",
        "* We can save the model weights only, the full model (including the weights and the architecture), and the optimizer state.\n",
        "* Its useful to remember that when we are training a model, the parameters we are updating during the training process are the weights at each layer of the model. Our aim is to train on data, labels pairs that mean we can predict effectively on unseen data using the weights we have trained. So it is these weights that are saved, optionally along with the model architecture. \n",
        "* Optimizer state. We haven't focussed too much on optimizers, but remember that this is the way that the model weights are updated. The size of the update is set by the (user defined) `learning rate`. When we save a model we can therefore save the `optimizer-state`, meaning we can continue training a loaded model from the state it was in when the model was saved.  \n",
        "\n",
        "**Why would we save only the model weights?**\n",
        "* Space, size? \n",
        "* Don't want to associate it with the model architecture for some reason? \n",
        "\n",
        "**Why save a model?**\n",
        "* Saves on training time \n",
        "* Deploy model \n",
        "* Train later\n",
        "\n",
        "**Restore**\n",
        "* Once we have saved our weights and/or model, we can restore the model in a couple of different ways. If we decide to save the weights only, we need to create an identical model to the one that was used to create our weights. If we saved both the model and weights, we can load this entire model.\n",
        "\n",
        "**Why do we need an identical model?**\n",
        "* Blah \n",
        "\n",
        "**What are the ways of doing it?**\n",
        "* In this tutorial (notebook 2), we will look at saving and loading model weights and model + weight manually, i.e. after training. \n",
        "* In notebook 3, we will look at how to save during and after training using callbacks. \n",
        "* We will use the Keras API. Note there are some other ways to save the model covered in the tensorflow tutorials "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OsFddwQCC6M",
        "colab_type": "text"
      },
      "source": [
        "#### 4.3.1 Saving and Loading Weights Only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQcYM1kpJJRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "model.save_weights('./checkpoints/my_checkpoint')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKXouUmZJKpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to load the weights we need to create a new instance of the same model architecture \n",
        "new_instance = sequential_model...\n",
        "\n",
        "# then we can load the weights \n",
        "new_instance.load_weights('./checkpoints/my_checkpoint')\n",
        "\n",
        "## perhaps extract a weight from one of the layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYCpb0ddCHVP",
        "colab_type": "text"
      },
      "source": [
        "#### 4.3.2 Saving and Loading an entire model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RO-cUlfCKmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## just look into this - might need to create a new instance here\n",
        "## or perhaps save new_instance and compare to above\n",
        "\n",
        "# this saves it to the HDF5 format\n",
        "model.save('my_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDAwnX8-DGZi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# recreate the saved model, including weights and optimizer ## where do i look to extract that?\n",
        "new_model = keras.models.load_model('my_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sms9gfHEVa1W",
        "colab_type": "text"
      },
      "source": [
        "**So, what did we cover in this section?**\n",
        "* Answer\n",
        "\n",
        "**How does it add to our existing knowledge?**\n",
        "* Answer\n",
        "\n",
        "**What else can I learn to improve my knowledge?**\n",
        "* Detail \n",
        "* Other things include..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqT0LadcXjut",
        "colab_type": "text"
      },
      "source": [
        "##5. Inference "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-DNj1kKP5yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load our model up \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLPJWTHgX286",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show how the model performs (re)using some code from previous notebook\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KPv43nP_Qan",
        "colab_type": "text"
      },
      "source": [
        "## 6. Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCzEXjhS_Snc",
        "colab_type": "text"
      },
      "source": [
        "**Did do:**\n",
        "* Training - validation, overfitting, \n",
        "\n",
        "**Didn't do:**\n",
        "* callbacks?"
      ]
    }
  ]
}